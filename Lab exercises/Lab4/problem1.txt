Дадено ни е податочно множество за соларен одблесок. Сите атрибути кои ги содржи се од категориски тип. Ваша задача е да истренирате класификатор - дрво на одлука кој ќе предвидува класи на соларен одблесок користејќи ги последните X% од даденото податочно множество. Треба да ја пресметате точноста која ја добивате над останатите (100 - X)% од податочното множество.

Во почетниот код имате дадено податочно множество. На влез се прима вредност за процентот на поделба X. На пример, ако вредноста е 80 значи дека ги користите последните 80% од множеството за тренирање, а првите 20% за тестирање. Дополнително во променливата criterion се вчитува вредност за критериумот за избор на најдобар атрибут.

На излез треба да се испечати точност, длабочина и број на листови на изграденото дрво, како и карактеристиките со најголема и најмала важност.

За да ги добиете истите резултати како и во тест примерите, при креирање на класификаторот поставете random_state=0

For example:
Test 	Input 	Result

TC1

	

50
entropy

	

Depth: 12
Number of leaves: 125
Accuracy: 0.7771345875542692
Most important feature: 0
Least important feature: 9

import os
os.environ['OPENBLAS_NUM_THREADS'] = '1'
from submission_script import *
from dataset_script import dataset
from sklearn.preprocessing import OrdinalEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

if __name__ == '__main__':
    # Read input values
    X_percent = int(input())
    criterion = input().strip()
    
    # Initialize encoder and fit on all features
    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
    encoder.fit([row[:-1] for row in dataset])  # Fit on all feature data
    
    # Separate features and target
    X = [row[:-1] for row in dataset]
    Y = [row[-1] for row in dataset]
    
    # Calculate split index (last X% for training)
    split_idx = int(len(dataset) * (100 - X_percent) / 100)
    
    # Split dataset
    train_X = X[split_idx:]
    train_Y = Y[split_idx:]
    test_X = X[:split_idx]
    test_Y = Y[:split_idx]
    
    # Transform features
    train_X_encoded = encoder.transform(train_X)
    test_X_encoded = encoder.transform(test_X)
    
    # Train classifier with fixed parameters
    classifier = DecisionTreeClassifier(
        criterion=criterion,
        random_state=0,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features=None
    )
    classifier.fit(train_X_encoded, train_Y)
    
    # Predict and calculate accuracy
    predictions = classifier.predict(test_X_encoded)
    accuracy = accuracy_score(test_Y, predictions)
    
    # Get tree metrics
    depth = classifier.get_depth()
    n_leaves = classifier.get_n_leaves()
    
    # Get feature importances
    importances = classifier.feature_importances_
    most_important = importances.argmax()
    least_important = importances.argmin()
    
    # Print results
    print(f"Depth: {depth}")
    print(f"Number of leaves: {n_leaves}")
    print(f"Accuracy: {accuracy}")
    print(f"Most important feature: {most_important}")
    print(f"Least important feature: {least_important}")
    
    # Submit the required components
    submit_train_data(train_X_encoded, train_Y)
    submit_test_data(test_X_encoded, test_Y)
    submit_classifier(classifier)
    submit_encoder(encoder)